---
title: "Cleaning r/biotech Salary Data"
format:
  html:
    toc: true
    toc-depth: 3
editor: visual
editor_options: 
  chunk_output_type: console
---

# libraries

```{r, message = FALSE, warning = FALSE}
library(readr)
library(here)
library(purrr)
library(glue)
library(lubridate)
library(dplyr)
library(tidyr)
library(gt)
library(ggplot2)
library(skimr)
library(janitor)
library(stringr)
library(tictoc)
library(usethis)
```

# preprocess excel data 

I extract the sheets into separate csv files so that they can display on github and be more easily used.

```{r process-sheets, eval = FALSE}
# run this once, manually
library(googlesheets4)
google_sheets_url <- 'https://docs.google.com/spreadsheets/d/1G0FmJhkOME_sv66hWmhnZS5qR2KMTY7nzkxksv46bfk/edit#gid=491268892'
sheet_names <- c('2024', '2023', '2022')

#read_sheet(google_sheets_url, sheet = '2024', col_types = 'c')

tic()
sheet_names |>  
  # read it as character values to avoid list-column headaches
  walk(
    \(x) read_sheet(google_sheets_url, sheet = x, col_types = 'c') |> 
      write_csv(here('data', glue::glue('{x}_survey_results.csv')))
  )
toc()
```

# Initial exploratory

## Completeness

Look at response rate per question (number of NAs):

```{r read_data}
# read in each sheet and combine
sal <- c('2022', '2023', '2024') %>% 
  setNames(., nm = .) |>   
  map(
    \(x) read_csv(
      here('data', glue::glue('{x}_survey_results.csv')), 
      col_types = cols(.default = "c"))
  )  |> 
  list_rbind(names_to = 'sheet_year')


# assess NAs
percent_na <- function(x, invert = FALSE) {
  p <- sum(is.na(x)) / length(x)
  if (invert) { p <- 1 - p}
  scales::percent(p)
}

sal |> 
  summarize(
    across(everything(), ~percent_na(.x, invert = TRUE))
  ) |> 
  pivot_longer(everything(), names_to = 'column', values_to = '% responded') |> 
  gt()
```

skimr has a great function that does this, `skim`:

```{r}
sal |>  skim()
```

## Remove those with 0 answers:

```{r}
# remove variables that are all NA:
var_all_na <- sal |>  
  skim()  |>   
  filter(n_missing == nrow(sal)) |> 
  pull(skim_variable)

message('removing variables that are have all missing values')
sal_clean <- sal |> 
  select(-any_of(var_all_na))
message(glue(
  "{length(var_all_na)}/{ncol(sal)} columns removed that are all NA"
))
print(var_all_na)
```

## Rename column names

I rename column names to make them easier to call:

```{r}
# view changed colnames
tibble(original = colnames(sal)) |> 
  mutate(new = janitor::make_clean_names(original)) |> 
  gt() 
sal_clean <- janitor::clean_names(sal_clean)
```

Now I begin cleaning the variable values

# Column cleaning

It would be a massive effort to clean every column. Let's prioritize to most important ones:

- Salary
- base
- target bonus (%)
- equity
- Title
- Experience
- years
- degree
- Location
- country
- city

If I have time, can look also at:

- 401k match

## salary

- base
- target bonus (%)
- equity

There are a handful of responses with annual salary reportedly less than $5000. There is one person that responded with "$1" - we remove them. And then the remaining report a range from 105-210, which likely represent 105**k** - 210**k**, I multiple these ones by 1000.

### base

```{r}
# make numeric
sal_clean <- sal_clean |>  
  mutate(
    across(
      c(compensation_annual_base_salary_pay, 
        compensation_annual_target_bonus), ~as.numeric(.x))
  )

sal_clean |>  
  ggplot(aes(x = compensation_annual_base_salary_pay)) + 
  geom_histogram() +
  scale_x_continuous(labels = scales::number)

# a bunch of salary entries that are <1000:
sal_clean |> 
  filter(compensation_annual_base_salary_pay < 5000) |> 
  count(compensation_annual_base_salary_pay)

# remove guy with "1" in base
sal_clean_filt <- sal_clean |> 
  filter(compensation_annual_base_salary_pay > 1)
n_removed <- nrow(sal_clean) - nrow(sal_clean_filt)

# for the remainder of individuals with salary < 5000 (all in hundreds), 
# multiple by 1000
sal_clean_filt <- sal_clean_filt |>  
  mutate(
    salary_base = ifelse(
      compensation_annual_base_salary_pay < 1000,
      compensation_annual_base_salary_pay*1000, 
      compensation_annual_base_salary_pay     
    ))

n_changed <- sal_clean_filt |> 
  filter(salary_base != compensation_annual_base_salary_pay) |>
  nrow() 
```

We removed `r n_removed` data point, and changed / cleaned `r n_changed` 

```{r}
sal_clean_filt |> 
  filter(salary_base != compensation_annual_base_salary_pay) |>
  select(compensation_annual_base_salary_pay, salary_base) |> 
  gt()
```

### target bonus

is a mix of % and $ amount


I combine this with annual base to create two new variables: bonus_pct, bonus (raw)

My strategy:

1. Remove special characters, except periods which represent decimals
2. convert to numeric, which has two impacts:
- "Not Applicable/None" entries will be converted to NAs
- percentages will be converted from those values that are less than "50"
- raw currency values will be simply those that are above 1000
3. If there is a range of values, then I keep the higher value, since bonus is typically interpreted as the maximum bonus amount, depending on company and individual performance


```{r}
sal_clean_filt |> 
  select(compensation_annual_base_salary_pay, 
         compensation_annual_target_bonus) |>  
  head(n = 20) |> gt() 

sal_clean_filt <- sal_clean_filt |> 
  mutate(
    bonus_cleaned = compensation_annual_target_bonus |> 
      
      # remove special characters
      str_remove_all('[\\$,\\%]') |>
      
      # remove any non-digits at beginning or end of string
      str_remove('^[^\\d]+') |> 
      str_remove('[^\\d]+$') |> 
      
      # if there is text between numbers, 
      # split it as likely this represents a range
      str_split('[^\\d\\.]+') |> 
      
      # convert to numeric
      # if it is a range, keep the highest value
      map_dbl(\(x) x |> as.numeric() |> max()) 
  ) |> 
  
  mutate(bonus_pct = case_when(
    bonus_cleaned < 50 ~ bonus_cleaned/100,
    bonus_cleaned > 1000 ~ bonus_cleaned / salary_base,
    TRUE ~ NA
  ),
  bonus = case_when(
    bonus_cleaned < 50 ~ bonus_pct * salary_base,
    bonus_cleaned > 1000 ~ bonus_cleaned,
    TRUE ~ NA
  ))
```

Let's look at the cleaned bonus data vs the original:


```{r}
sal_clean_filt |> 
  
  # take 12 rows, 6 that are different, and then 6 not
  mutate(different = compensation_annual_target_bonus != bonus_cleaned) |> 
  slice_sample(n = 6, by = different) |>  
  
  count(salary_base, compensation_annual_target_bonus, 
        bonus_cleaned, bonus_pct, bonus)  |> 
  arrange(salary_base) |> 
  gt() |> 
  fmt_percent(bonus_pct) |> 
  fmt_currency(c(salary_base, bonus))


n_changed_bonus <- sal_clean_filt |> 
  filter(salary_base != compensation_annual_base_salary_pay) |>
  nrow() 
```

I converted bonus into % and raw $. This involved modifying `r n_changed_bonus` bonus data points.

### equity

NOT CURRENTLY ATTEMPTED. Prioritize other data first.

Equity is a little bit complicated:

Some are reported as options, some are reported in 1000s e.g. "15k" vs "15000"
Some are reported in $ amount
Some say "it varies per year" 
There are a few that report %, is that amount percentage from base in $ or in options??


1. We set "Not Applicable/None" to 0
2. Set responses that report %, to NA (I don't want to deal with this and it is a small number of responses)
2. We then create two variables, equity_options, equity_currency_value

- `equity_currency_value` takes those data points that start with a dollar sign $
- `equity_options` is everything else
- treat this the same as base salary:
- clean and then:
- where if it is less than 1000, multiple by 1000

3. Convert everything to numeric

```{r}
sal_clean_filt |> 
  pull(compensation_annual_equity_stock_option) |>  
  unique() |> 
  head(n = 20) 
```

## Title

Title is very important, but also very messy. Titles can have different meanings (compensation, experience, responsibilities) between companies, and location.

```{r}
sal_clean_filt |> 
  count(role_title_of_current_position) |>
  slice_sample(n=10) |> 
  gt() 
```

### filtering
Remove some obvious rows

```{r}
sal_clean_filt <- sal_clean_filt |> 
  filter(role_title_of_current_position != 'pimp')
```



### Standardize titles

It's a lot of work to standardize titles, so I focus on what I know best, scientist 

#### Scientist

It's a lot of work to standardize titles, so I focus on what I know best, scientist 

Scientist:

- Research Associate
- Associate Scientist
- Scientist
- Senior Scientist
- Staff Scientist
- Principal Scientist

I'm going to create two new variables

- `title_category` - Set this to `Scientist`
- `title_general` - A variable that includes general scientist + levels, e.g. Sci **III**, and **Associate** Sci
- `title_detail` - Contains the other detail of job title, e.g. **Research**, **Clinical**

```{r}
# all roles matching "Scientist"
sal_clean_filt |> 
  filter(str_detect(
    role_title_of_current_position, regex('Scientist', ignore_case = FALSE))) |> 
  count(role_title_of_current_position) |> head(25) |> gt()
```


Extract **levels** + *general title* + `sub-level` = **Associate** *Scientist* `I`

- There's a lot of "Sr" prefixes, let's replace those with "Senior"
- Normalize  sub-levels, 1 -> I, 2 -> II
- Pattern Match to level + general title + subtitle
- put other detail into another variable

```{r}
title_clean_sci <- sal_clean_filt |> 
  filter(str_detect(
    role_title_of_current_position, regex('Scientist', ignore_case = TRUE))) |>
  mutate(
    title_category = 'Scientist',
    title_ = tolower(role_title_of_current_position) |>  
      
      # address single cases that are hard to generalize
      str_remove('\\(r') |> # a single case
      str_replace('scientist ll', 'Scientist II') |>  # someone used "L"s LOL
      
      
      # Replace sr with senior
      str_replace_all('sr\\.?', 'Senior') |> 
      str_replace_all('1', 'I') |> 
      str_replace_all('2', 'II') |>
      str_replace_all('3', 'III'),
    
    # Extract the "level" part of title
    title_general = str_extract(
      title_,
      '((associate|senior|principal|staff)?\\s?(data\\s?)?)*scientist\\s?[iI]*'
    ) |> 
      str_trim(),
    title_general = ifelse(title_general == '', NA, title_general),
    title_general = title_general |> str_remove('\\s[iI]+$') |> 
      str_to_title(), 
    
    # Put everything else into another variable
    title_detail = 
      title_ |> 
      str_remove('scientist\\s?[iI]*') |> 
      str_remove_all('[()-]') |> 
      str_remove_all('(associate|senior|principal|staff)') |> 
      str_replace_all('/', ' ') |> 
      str_trim(),
    title_detail = ifelse(title_detail == '', NA, title_detail),
    title_detail = title_detail |>  
      str_to_title() |> 
      forcats::fct() |> 
      forcats::fct_na_value_to_level(level = 'None')
  ) 

title_clean_sci |> 
  count(title_general, title_detail, role_title_of_current_position) |> 
  arrange(role_title_of_current_position) |>  slice_sample(n = 25) |> gt() 

# join back to data
title_clean <- sal_clean_filt |> 
  
  filter(!str_detect(
    role_title_of_current_position, regex('Scientist', ignore_case = TRUE))) |>
  bind_rows(title_clean_sci)

stopifnot(nrow(title_clean) == nrow(sal_clean_filt))
glimpse(title_clean |>  select(contains('title')))
```

#### Directors and VP

Same approach as for Scientist, but these are the levels:

```{r}
sal_clean_filt |> 
  filter(str_detect(
    role_title_of_current_position, regex('Director', ignore_case = TRUE))) |> 
  count(role_title_of_current_position) |>  slice_sample(n = 25) |> gt()
```

- Associate Director
- Director
- Senior Director
- Executive Director
- VP

```{r}
s_title <- '(Dir(ector)?|VP|AD)'
s_levels <- '(Associate|Senior|Executive|Principal|Staff)'

title_clean_dv <- title_clean |> 
  filter(str_detect(
    role_title_of_current_position, 
    regex(s_title, ignore_case = TRUE)) |
      str_detect(role_title_of_current_position, 'AD')
      ) |>
  mutate(
    title_category = role_title_of_current_position |> 
      str_extract(regex(s_title, ignore_case = TRUE)),
    title_ = role_title_of_current_position |>  
      
      # address single cases that are hard to generalize
      str_replace('ad', 'associate director') |> 
      str_replace('vp', 'VP') |> 
      str_replace('SVP', 'Senior VP') |> 
      str_replace('clin ops', 'Clinical Operations') |> 
      str_replace('operations', 'Operations') |> 
      # Replace sr with senior
      str_replace_all(regex('Sr\\.?', ignore_case = TRUE), 'Senior'),
    
    # Extract the "level" part of title
    title_general = str_extract(title_, glue('({s_levels}?\\s?)*{s_title}')) |> 
      str_trim(),
    
    # Put everything else into another variable
    title_detail = 
      title_ |> 
      str_remove(s_title) |> 
      str_remove_all(s_levels) |> 
      str_remove_all(',') |> 
      str_replace_all('/', ' ') |> 
      str_trim() |> 
      str_remove('^of ') |> 
      str_replace("^\\w{1}", toupper),
    title_general = ifelse(title_general == '', NA, title_general),
    title_detail = ifelse(title_detail == '', NA, title_detail) |> 
      forcats::fct() |>  forcats::fct_na_value_to_level('None')
  ) 

title_clean_dv |> 
  count(title_category, title_general, title_detail, role_title_of_current_position) |> 
  arrange(role_title_of_current_position) |>  slice_sample(n = 25) |> gt() 
```

Now join back 

```{r}
title_clean_sdv <- title_clean |> 
  
  # remove the dir / vp rows that we filled in
  filter(!str_detect(
    role_title_of_current_position, regex(s_title, ignore_case = TRUE))) |>
  
  # then add them back in
  bind_rows(title_clean_dv)

# check results
stopifnot(nrow(title_clean_sdv) == nrow(title_clean))
title_clean_sdv |>  select(contains('title')) |>  glimpse()

title_clean_sdv |> 
  count(title_category, title_general, title_detail) |> 
  slice_sample(n = 25) |> 
  gt()

title_clean_sdv |>  count(title_category)
```

### Roles that have not been processed

Look at the remaining roles that I have not tried to process.


```{r}
title_clean_sdv |>  
  filter(is.na(title_category)) |>  count(role_title_of_current_position) |> 
  gt()
```


### final

The non-processed data is useful, if messy. Let's populate the cleaned variables with values from the non cleaned responses.

```{r}
sal_clean_filt <- title_clean_sdv |> 
  mutate(
    title_category = ifelse(
      is.na(title_category), 'Other', title_category),
    title_general = ifelse(
      is.na(title_general), 
      
      role_title_of_current_position |> 
        str_replace("^\\w{1}", toupper) |> # first 
        str_replace_all('\\b+\\w{1}', toupper),  # every other
      
      title_general),
    title_detail = ifelse(
      is.na(title_detail), role_title_of_current_position, title_detail)
  ) 
```

## Experience

```{r}
sal_clean_filt |> 
  count(years_of_experience)

sal_clean_filt |> 
  count(what_degrees_do_you_have)
```


## Location

Location is quite messy data consisting of free-form responses  spread between 3 variables:

- `where_are_you_located` contains Countries (USA, Canada, etc.), states (CO, Carolinas, etc.)

This is not a required question so there are 174 responses that did not respond, and this also included an "other" freeform option.

- Pharma Central (NY, NJ, PA)
- New England (MA, CT, RI, NH, VT, ME)
- DC Metro Area (DC, VA, MD, DE)
- Carolinas & Southeast (From NC to AR, South FL and LA)
- Midwest (From OH to KS, North to ND)
- South & Mountain West (TX to AZ, North to MT)
- West Coast (California & Pacific Northwest)
- Other US Location (HI, AK, PR, etc.)
- Canada
- United Kingdom and Ireland
- Germany
- Other:


The 174 missing responses have values for these other two variables. I think this is due to the survey changing the question at some point, probably due to feedback.

- `what_country_do_you_work_in` seems to all be countries, but not harmonized e.g.: Usa, usa, United States
- `where_is_the_closest_major_city_or_hub` mixed bag: Bay area, Boston, san diego, Research Triangle, 

Here I create one new location variable `location_country`, that, unlike the other variables, cover >90% of the data

```{r}
USA <- c(
  "Pharma Central (NY, NJ, PA)",
  "New England (MA, CT, RI, NH, VT, ME)",
  "DC Metro Area (DC, VA, MD, DE)",
  "Carolinas & Southeast (From NC to AR, South FL and LA)",
  "Midwest (From OH to KS, North to ND)",
  "South & Mountain West (TX to AZ, North to MT)",
  "West Coast (California & Pacific Northwest)",
  "Other US Location (HI, AK, PR, etc.)"
)

sal_clean_filt <- sal_clean_filt |> mutate(
  location_raw_data = ifelse(
    is.na(where_are_you_located), 
    what_country_do_you_work_in, 
    where_are_you_located) |> 
    tolower(),
  location_country = case_when(
    location_raw_data %in% c(
      tolower(USA), 'co', 'san diego, ca', 'united states') ~ 'United States of America',
    str_detect(location_raw_data, regex('usa?', ignore_case = TRUE)) ~ 
      'United States of America',
    
    location_raw_data %in% c(
      'united kingdom and ireland', 'uk') ~ 'United Kingdom',
    
    
    location_raw_data %in% 
      c('canada', 'united states of america',
        'belgium', 'france', 'spain', 'united kingdom', 'benelux', 'germany',
        'sweden', 'switzerland', 'denmark',
        'singapore', 'india', 'australia') ~ location_raw_data,
    TRUE ~ NA
    
  ),
  location_country = str_to_title(location_country)
)

sal_clean_filt |> 
  count(location_country, location_raw_data) |> 
  arrange(location_country) |>  gt()
```

```{r}
sal_clean_filt |> 
  count(
    where_are_you_located, what_country_do_you_work_in, where_is_the_closest_major_city_or_hub) |>  gt()

```

### Completeness

Compare the missingness of original location variables vs the cleaned `location_country` variable:

```{r}
sal_clean_filt |> 
  summarize(
    across(c(
      location_country, 
      where_are_you_located, 
      what_country_do_you_work_in, 
      where_is_the_closest_major_city_or_hub), 
      ~ percent_na(.x))
  ) |> 
  pivot_longer(everything(), names_to = 'column', values_to = '% missing')
```

# time

```{r}
sal_clean_filt <- sal_clean_filt |> 
  mutate(timestamp = mdy_hms(timestamp),
         year = year(timestamp),
         month = month(timestamp),
         day = day(timestamp)) |> 
  select(timestamp, year:day, everything())
```


# write out the data

```{r}
sal_clean_filt |>
  mutate(date = str_extract(as.character(timestamp), "^[-\\w]+")) |> 
  select(
    date, 
    location_country, 
    title_category, title_general, title_detail,
    salary_base, bonus_pct, bonus,
    
    # need to fix these
    years_of_experience, what_degrees_do_you_have,
    
    # the original data
    where_are_you_located, what_country_do_you_work_in,
    role_title_of_current_position,
    everything(),
    -timestamp, -title_, -bonus_cleaned, -year, -month, -day
  ) |> 
  write_csv(here::here('data', 'salary_results_cleaned.csv'))
```

```{r eval = TRUE}
salaries <- read_csv(here::here('data', 'salary_results_cleaned.csv'))
use_data(salaries, overwrite = TRUE)
```
