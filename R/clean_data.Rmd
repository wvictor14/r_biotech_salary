---
title: "Clean 2023 salary data"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r}
library(readr)
library(here)
library(purrr)
library(glue)
library(lubridate)
library(dplyr)
library(tidyr)
library(gt)
library(ggplot2)
library(skimr)
library(janitor)
library(stringr)
library(tictoc)
```

# preprocess excel data 

I extract the sheets into separate csv files so that they can display on github and be more easily used.

```{r process-sheets, eval = FALSE}
# run this once, manually
library(googlesheets4)
google_sheets_url <- 'https://docs.google.com/spreadsheets/d/1G0FmJhkOME_sv66hWmhnZS5qR2KMTY7nzkxksv46bfk/edit#gid=491268892'
sheet_names <- c('2024', '2023', '2022')

#read_sheet(google_sheets_url, sheet = '2024', col_types = 'c')

tic()
sheet_names |>  
  # read it as character values to avoid list-column headaches
  walk(
    \(x) read_sheet(google_sheets_url, sheet = x, col_types = 'c') |> 
      write_csv(here('data', glue::glue('{x}_survey_results.csv')))
  )
toc()
```

# 2024 data

load and clean

Look at response rate per question (number of NAs):

```{r}
sal <- read_csv(here('data', '2024_survey_results.csv'))
sal |>  skim()
```

Remove those with 0 answers:

```{r}
# remove variables that are all NA:
var_all_na <- sal |>  
  skim()  |>   
  filter(n_missing == nrow(sal)) |> 
  pull(skim_variable)

message('removing variables that are have all missing values')
sal_clean <- sal |> 
  select(-any_of(var_all_na))
message(glue(
  "{length(var_all_na)}/{ncol(sal)} columns removed that are all NA"
))
print(var_all_na)
```

I rename column names to make them easier to call:

```{r}
# view changed colnames
tibble(original = colnames(sal)) |> 
  mutate(new = janitor::make_clean_names(original)) |> 
  gt() 
sal_clean <- janitor::clean_names(sal_clean)
```

Now I begin cleaning the variable values

## Data cleaning

It would be a massive effort to clean every column. Let's prioritize to most important ones:

- Salary
- base
- target bonus (%)
- equity
- Title
- Experience
- years
- degree
- Location
- country
- city

If I have time, can look also at:

- 401k match

## salary

- base
- target bonus (%)
- equity

There are a handful of responses with annual salary reportedly less than $5000. There is one person that responded with "$1" - we remove them. And then the remaining report a range from 105-210, which likely represent 105**k** - 210**k**, I multiple these ones by 1000.

### base

```{r}
sal_clean |>  
  ggplot(aes(x = compensation_annual_base_salary_pay)) + 
  geom_histogram() +
  scale_x_continuous(labels = scales::number)

# a bunch of salary entries that are <1000:
sal_clean |> 
  filter(compensation_annual_base_salary_pay < 5000) |> 
  count(compensation_annual_base_salary_pay)

# remove guy with "1" in base
sal_clean_filt <- sal_clean |> 
  filter(compensation_annual_base_salary_pay > 1)
n_removed <- nrow(sal_clean) - nrow(sal_clean_filt)

# for the remainder of individuals with salary < 5000 (all in hundreds), 
# multiple by 1000
sal_clean_filt <- sal_clean_filt |>  
  mutate(
    salary_base = ifelse(
      compensation_annual_base_salary_pay < 1000,
      compensation_annual_base_salary_pay*1000, 
      compensation_annual_base_salary_pay     
    ))

n_changed <- sal_clean_filt |> 
  filter(salary_base != compensation_annual_base_salary_pay) |>
  nrow() 
```

We removed `r n_removed` data point, and changed / cleaned `r n_changed` 

```{r}
sal_clean_filt |> 
  filter(salary_base != compensation_annual_base_salary_pay) |>
  select(compensation_annual_base_salary_pay, salary_base) |> 
  gt()
```

### target bonus

is a mix of % and $ amount


I combine this with annual base to create two new variables: bonus_pct, bonus (raw)

My strategy:

1. Remove special characters, except periods which represent decimals
2. convert to numeric, which has two impacts:
- "Not Applicable/None" entries will be converted to NAs
- percentages will be converted from those values that are less than "50"
- raw currency values will be simply those that are above 1000
3. If there is a range of values, then I keep the higher value, since bonus is typically interpreted as the maximum bonus amount, depending on company and individual performance


```{r}
sal_clean_filt |> 
  select(compensation_annual_base_salary_pay, 
         compensation_annual_target_bonus) |>  
  head(n = 20) |> gt() 

sal_clean_filt <- sal_clean_filt |> 
  mutate(
    bonus_cleaned = compensation_annual_target_bonus |> 
      
      # remove special characters
      str_remove_all('[\\$,\\%]') |>
      
      # remove any non-digits at beginning or end of string
      str_remove('^[^\\d]+') |> 
      str_remove('[^\\d]+$') |> 
      
      # if there is text between numbers, 
      # split it as likely this represents a range
      str_split('[^\\d\\.]+') |> 
      
      # convert to numeric
      # if it is a range, keep the highest value
      map_dbl(\(x) x |> as.numeric() |> max()) 
  ) |> 
  
  mutate(bonus_pct = case_when(
    bonus_cleaned < 50 ~ bonus_cleaned/100,
    bonus_cleaned > 1000 ~ bonus_cleaned / salary_base,
    TRUE ~ NA
  ),
  bonus = case_when(
    bonus_cleaned < 50 ~ bonus_pct * salary_base,
    bonus_cleaned > 1000 ~ bonus_cleaned,
    TRUE ~ NA
  ))
```

Let's look at the cleaned bonus data vs the original:

```{r}
sal_clean_filt |> 
  filter(compensation_annual_target_bonus != bonus_cleaned) |> 
  count(salary_base, compensation_annual_target_bonus, 
        bonus_cleaned, bonus_pct, bonus)  |> 
  arrange(salary_base) |> 
  gt() |> 
  fmt_percent(bonus_pct) |> 
  fmt_currency(c(salary_base, bonus))


n_changed_bonus <- sal_clean_filt |> 
  filter(salary_base != compensation_annual_base_salary_pay) |>
  nrow() 
```

I converted bonus into % and raw $. This involved modifying `r n_changed_bonus` bonus data points.

### equity

Equity is a little bit complicated:

Some are reported as options, some are reported in 1000s e.g. "15k" vs "15000"
Some are reported in $ amount
Some say "it varies per year" 
There are a few that report %, is that amount percentage from base in $ or in options??


1. We set "Not Applicable/None" to 0
2. Set responses that report %, to NA (I don't want to deal with this and it is a small number of responses)
2. We then create two variables, equity_options, equity_currency_value

- `equity_currency_value` takes those data points that start with a dollar sign $
- `equity_options` is everything else
- treat this the same as base salary:
- clean and then:
- where if it is less than 1000, multiple by 1000

3. Convert everything to numeric

```{r}
sal_clean_filt |> 
  pull(compensation_annual_equity_stock_option) |>  
  unique() |> 
  head(n = 20) 
```


## Title

It's a lot of work to standardize titles, so I focus on what I know best, scientist 

```{r}
sal_clean_filt |> 
  count(role_title_of_current_position) |>
  slice_sample(n=10) |> 
  gt() 
```

I'm going to create two new variables

A variable that includes general scientist levels
And another that has the other detail of job title, such as Scientist of Research


```{r}
# all roles matching "Scientist"
sal_clean_filt |> 
  filter(str_detect(
    role_title_of_current_position, regex('Scientist', ignore_case = FALSE))) |> 
  count(role_title_of_current_position) |> gt()
```


There's a lot of "Sr" prefixes, let's replace those with "Senior"

```{r}
sal_title_clean_sci <- sal_clean_filt |> 
  filter(str_detect(
    role_title_of_current_position, regex('Scientist', ignore_case = FALSE))) |>
  mutate(
    title_ = role_title_of_current_position |>  
      
      # address single cases that are hard to generalize
      str_remove('\\(R') |> # a single case
      str_replace('Scientist ll', 'Scientist II') |>  # someone used "L"s LOL
      
      
      # Replace sr with senior
      str_replace_all(regex('Sr\\.?', ignore_case = TRUE), 'Senior') |> 
      str_replace_all('1', 'I') |> 
      str_replace_all('2', 'II') |>
      str_replace_all('3', 'III'),
    
    # Extract the "level" part of title
    title_standard = str_extract(
      title_,
      '((Associate|Senior|Principal)?\\s?(Data\\s?)?)*Scientist\\s?[I]*'
      ),
    
    # Put everything else into another variable
    title_detail = 
      title_ |> 
      str_remove('Scientist\\s?[I]*') |> 
      str_remove_all('[()-]') |> 
      str_remove_all('(Associate|Senior|Principal)') |> 
      str_replace_all('/', ' ')
  ) 

sal_title_clean_sci |> 
  count(title_standard, title_detail, role_title_of_current_position) |> 
  arrange(role_title_of_current_position) |> gt() 
```


I create a new variable `title_standard` that categories responses into. And then I create a broader grouping variable:

Scientist:
- Research Associate
- Associate Scientist
- Scientist
- Senior Scientist
- Staff Scientist
- Principal Scientist
- Associate Director
- Director
- Senior Director
- Executive Director
- VP





```{r}

```

## Experience

```{r}
sal_clean_filt |> 
  count(years_of_experience)

sal_clean_filt |> 
  count(what_degrees_do_you_have)
```


## Location

Location is quite messy, are free-form responses  spread between 3 variables:

- `where_are_you_located` contains Countries (USA, Canada, etc.), states (CO, Carolinas, etc.)

This is not a required question so there are 174 responses that did not respond, and this also included an "other" freeform option.

- Pharma Central (NY, NJ, PA)
- New England (MA, CT, RI, NH, VT, ME)
- DC Metro Area (DC, VA, MD, DE)
- Carolinas & Southeast (From NC to AR, South FL and LA)
- Midwest (From OH to KS, North to ND)
- South & Mountain West (TX to AZ, North to MT)
- West Coast (California & Pacific Northwest)
- Other US Location (HI, AK, PR, etc.)
- Canada
- United Kingdom and Ireland
- Germany
- Other:


The 174 missing responses have values for these other two variables. I think this is due to the survey changing the question at some point, probably due to feedback.

- `what_country_do_you_work_in` seems to all be countries, but not harmonized e.g.: Usa, usa, United States
- `where_is_the_closest_major_city_or_hub` mixed bag: Bay area, Boston, san diego, Research Triangle, 




We can use tmaptools to separate this out into 3 new variables:

Country, State, City

I write a function to predict and extract structured location information based on the mess that is the current location variables are. It relies on the package `tmaptools` that uses the open source API.

I pings REST APIs so I try to speed this up by parallelizing using futures + furrr, but unfortunately it is stil quite slow for ~500 requests. So I run this once interactively, and then cache and use those results for the reporting.

```{r eval = FALSE}


file <- here::here('data', 'geo_queried_results.csv')
if (!file.exists(file)) {
  library(tmaptools)
  library(furrr)
  library(tictoc)
  
  #' @param location messy location variable in .df
  get_location <- function(.df, location) {
    possibly_geocode <- purrr::possibly(
      .f = ~
        tmaptools::geocode_OSM(.x, keep.unfound = TRUE, as.sf = TRUE) |> 
        tmaptools::rev_geocode_OSM(as.data.frame = TRUE) |>  
        as_tibble() |> 
        select(country, country_code, county, state),
      otherwise = data.frame(country = NA) # errors return an empty row 
    )
    
    .df |> 
      mutate(.results = furrr::future_map(
        {{location}}, possibly_geocode,
        .options = furrr_options(scheduling = 8L, seed = TRUE))
      ) |> 
      unnest(cols = c(.results))
  }
  
  plan(multisession, workers = 8L)
  #plan(sequential)
  tic();query <- sal_clean_filt |> 
    filter(!is.na(where_are_you_located)) |> 
    select(timestamp, where_are_you_located) |> 
    get_location(where_are_you_located);toc()
  
  ticlog <- tic.log(format = TRUE)
  fs::dir_create(here::here('log'))
  write_lines(ticlog, here::here('log', 'ticlog.log'))
  
  write_csv(query, file)
} else {
  query <- read_csv(file)
}
query 
```

It seems that it worked OK, but honestly it didn't get a number of responses that I thought would be easy. At least, the output is normalized so that saves me quite a bit of work.

```{r}
sal_clean_filt |>  
  left_join(query, by = c('where_are_you_located'))
```



```{r}
sal_clean_filt |> 
  count(where_are_you_located)
```



```{r}

sal_clean_filt |>  
  count(
    where_are_you_located, 
    what_country_do_you_work_in, 
    where_is_the_closest_major_city_or_hub) |>
  gt()


sal_clean_filt |> 
  count(where_are_you_located)

sal_clean_filt |> 
  count(what_country_do_you_work_in)

sal_clean_filt |> 
  count(where_is_the_closest_major_city_or_hub)
```

# time

```{r}
sal_clean_filt <- sal_clean_filt |> 
  mutate(timestamp = mdy_hms(timestamp),
         year = year(timestamp),
         month = month(timestamp),
         day = day(timestamp)) |> 
  select(timestamp, year:day, everything())
```


# write out the data

```{r}
sal_clean_filt |> 
  write_csv(here::here('data', 'salary_results_cleaned.csv'))
```


```{r eval = FALSE}
use_data(salaries, overwrite = TRUE)
```
