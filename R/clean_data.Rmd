---
title: "Clean 2023 salary data"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r}
library(readr)
library(here)
library(purrr)
library(glue)

library(dplyr)
library(tidyr)
library(gt)
library(ggplot2)
library(skimr)
library(janitor)
library(stringr)
```

# preprocess excel data 

I extract the sheets into separate csv files so that they can display on github and be more easily used.

```{r process-sheets, eval = FALSE}
# run this once, manually
library(googlesheets4)
google_sheets_url <- 'https://docs.google.com/spreadsheets/d/1G0FmJhkOME_sv66hWmhnZS5qR2KMTY7nzkxksv46bfk/edit#gid=491268892'
sheet_names <- c('2024', '2023', '2022')

sheet_names |>  
  # read it as character values to avoid list-column headaches
  walk(
    \(x) read_sheet(google_sheets_url, sheet = x, col_types = 'c') |> 
      write_csv(here('data', glue::glue('{x}_survey_results.csv')))
  )

```

# 2024 data

load and clean

Look at response rate per question (number of NAs):

```{r}
sal <- read_csv(here('data', '2024_survey_results.csv'))
sal |>  skim()
```

Remove those with 0 answers:

```{r}
# remove variables that are all NA:
var_all_na <- sal |>  
  skim()  |>   
  filter(n_missing == nrow(sal)) |> 
  pull(skim_variable)

message('removing variables that are have all missing values')
sal_clean <- sal |> 
  select(-any_of(var_all_na))
message(glue(
  "{length(var_all_na)}/{ncol(sal)} columns removed that are all NA"
))
print(var_all_na)
```

I rename column names to make them easier to call:

```{r}
# view changed colnames
tibble(original = colnames(sal)) |> 
  mutate(new = janitor::make_clean_names(original)) |> 
  gt() 
sal_clean <- janitor::clean_names(sal_clean)
```

Now I begin cleaning the variable values

## Data cleaning

It would be a massive effort to clean every column. Let's prioritize to most important ones:

- Salary
  - base
  - target bonus (%)
  - equity
- Title
- Experience
  - years
  - degree
- Location
  - country
  - city
  
If I have time, can look also at:

- 401k match

## salary

- base
- target bonus (%)
- equity

There are a handful of responses with annual salary reportedly less than $5000. There is one person that responded with "$1" - we remove them. And then the remaining report a range from 105-210, which likely represent 105**k** - 210**k**, I multiple these ones by 1000.

### base

```{r}
sal_clean |>  
  ggplot(aes(x = compensation_annual_base_salary_pay)) + 
  geom_histogram() +
  scale_x_continuous(labels = scales::number)

# a bunch of salary entries that are <1000:
sal_clean |> 
  filter(compensation_annual_base_salary_pay < 5000) |> 
  count(compensation_annual_base_salary_pay)

# remove guy with "1" in base
sal_clean_filt <- sal_clean |> 
  filter(compensation_annual_base_salary_pay > 1)
n_removed <- nrow(sal_clean) - nrow(sal_clean_filt)

# for the remainder of individuals with salary < 5000 (all in hundreds), 
# multiple by 1000
sal_clean_filt <- sal_clean_filt |>  
  mutate(
    salary_base = ifelse(
      compensation_annual_base_salary_pay < 1000,
      compensation_annual_base_salary_pay*1000, 
      compensation_annual_base_salary_pay     
    ))

n_changed <- sal_clean_filt |> 
    filter(salary_base != compensation_annual_base_salary_pay) |>
    nrow() 
```

We removed `r n_removed` data point, and changed / cleaned `r n_changed` 

```{r}
sal_clean_filt |> 
  filter(salary_base != compensation_annual_base_salary_pay) |>
  select(compensation_annual_base_salary_pay, salary_base) |> 
  gt()
```

### target bonus

is a mix of % and $ amount


I combine this with annual base to create two new variables: bonus_pct, bonus (raw)

My strategy:

1. Remove special characters, except periods which represent decimals
2. convert to numeric, which has two impacts:
  - "Not Applicable/None" entries will be converted to NAs
  - percentages will be converted from those values that are less than "50"
  - raw currency values will be simply those that are above 1000
3. If there is a range of values, then I keep the higher value, since bonus is typically interpreted as the maximum bonus amount, depending on company and individual performance


```{r}
sal_clean_filt |> 
  select(compensation_annual_base_salary_pay, 
         compensation_annual_target_bonus) |>  
  head(n = 20) |> gt() 

sal_clean_filt <- sal_clean_filt |> 
  mutate(
    bonus_cleaned = compensation_annual_target_bonus |> 
      
      # remove special characters
      str_remove_all('[\\$,\\%]') |>
      
      # remove any non-digits at beginning or end of string
      str_remove('^[^\\d]+') |> 
      str_remove('[^\\d]+$') |> 
      
      # if there is text between numbers, 
      # split it as likely this represents a range
      str_split('[^\\d\\.]+') |> 
      
      # convert to numeric
      # if it is a range, keep the highest value
      map_dbl(\(x) x |> as.numeric() |> max()) 
    ) |> 
  
  mutate(bonus_pct = case_when(
    bonus_cleaned < 50 ~ bonus_cleaned/100,
    bonus_cleaned > 1000 ~ bonus_cleaned / salary_base,
    TRUE ~ NA
  ),
  bonus = case_when(
    bonus_cleaned < 50 ~ bonus_pct * salary_base,
    bonus_cleaned > 1000 ~ bonus_cleaned,
    TRUE ~ NA
  ))
```

Let's look at the cleaned bonus data vs the original:

```{r}
sal_clean_filt |> 
  filter(compensation_annual_target_bonus != bonus_cleaned) |> 
  count(salary_base, compensation_annual_target_bonus, 
        bonus_cleaned, bonus_pct, bonus)  |> 
  arrange(salary_base) |> 
  gt() |> 
  fmt_percent(bonus_pct) |> 
  fmt_currency(c(salary_base, bonus))


n_changed_bonus <- sal_clean_filt |> 
    filter(salary_base != compensation_annual_base_salary_pay) |>
    nrow() 
```

I converted bonus into % and raw $. This involved modifying `r n_changed_bonus` bonus data points.

### equity

Equity is a little bit complicated:

Some are reported as options, some are reported in 1000s e.g. "15k" vs "15000"
Some are reported in $ amount
Some say "it varies per year" 
There are a few that report %, is that amount percentage from base in $ or in options??


1. We set "Not Applicable/None" to 0
2. Set responses that report %, to NA (I don't want to deal with this and it is a small number of responses)
2. We then create two variables, equity_options, equity_currency_value

- `equity_currency_value` takes those data points that start with a dollar sign $
- `equity_options` is everything else
  - treat this the same as base salary:
  - clean and then:
  - where if it is less than 1000, multiple by 1000

3. Convert everything to numeric

```{r}
sal_clean_filt |> 
  pull(compensation_annual_equity_stock_option) |>  
  unique() |> 
  head(n = 20) 
```


## Title

For title let's try to standardize some positions. I create a new variable `title_standard` that categories responses into:

- Research Associate
- Associate Scientist
- Scientist
- Senior Scientist
- Staff Scientist
- Principal Scientist
- Associate Director
- Director
- Senior Director
- Executive Director
- VP

```{r}
sal_clean_filt |> 
  count(role_title_of_current_position) |> 
  gt()
```

## Experience

```{r}
sal_clean_filt |> 
  count(years_of_experience)

sal_clean_filt |> 
  count(what_degrees_do_you_have)
```


## Location

Location is quite messy, are free-form responses  spread between 3 variables:

- `where_are_you_located` contains Countries (USA, Canada, etc.), states (CO, Carolinas, etc.)
- `what_country_do_you_work_in` seems to all be countries, but not harmonized e.g.: Usa, usa, United States
- `where_is_the_closest_major_city_or_hub` mixed bag: Bay area, Boston, san diego, Research Triangle, 

We can use tmaptools to separate this out into 3 new variables:

Country, State, City

I write a function to 

```{r}
library(tmaptools)

query <- sal_clean_filt |> 
  filter(!is.na(where_are_you_located)) |> 
  slice(1:6) |>
  select(where_are_you_located) 

possibly_geocode <- possibly(
  .f = ~tmaptools::geocode_OSM(.x, keep.unfound = TRUE, as.sf = TRUE) |> 
    tmaptools::rev_geocode_OSM(as.data.frame = TRUE) |>  
    as_tibble() |> 
    select(country, country_code, county, state),
  otherwise = NULL
) 

geo <- query$where_are_you_located %>%
  setNames(., nm = .) |> 
  map(possibly_geocode) |> 
  list_rbind(names_to = 'query')

sal_clean_filt |> 
  left_join(
    geo,
    by = c('where_are_you_located' = 'query')
  )

```




```{r}

sal_clean_filt |>  
  count(
    where_are_you_located, 
    what_country_do_you_work_in, 
    where_is_the_closest_major_city_or_hub) |>
  gt()


sal_clean_filt |> 
  count(where_are_you_located)

sal_clean_filt |> 
  count(what_country_do_you_work_in)

sal_clean_filt |> 
  count(where_is_the_closest_major_city_or_hub)
```

